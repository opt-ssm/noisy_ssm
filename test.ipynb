{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import sys\n",
    "from contextlib import contextmanager\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sapr7y/noisy_ssm/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-09-02 20:36:41.199368: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-02 20:36:41.218639: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-02 20:36:41.224450: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-02 20:36:41.237852: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-02 20:36:42.423365: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/sapr7y/noisy_ssm/.venv/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='162' max='12948' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  162/12948 00:27 < 36:11, 5.89 it/s, Epoch 0.05/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch, os\n",
    "from transformers import AutoTokenizer, TrainingArguments, Trainer\n",
    "from accelerate import Accelerator\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "# wandb.init(mode=\"disabled\")\n",
    "\n",
    "modelpath=\"state-spaces/mamba-1.4b\"\n",
    "bs=4        # batch size\n",
    "ga_steps=1  # gradient acc. steps\n",
    "epochs=4\n",
    "lr=0.00005\n",
    "output_dir=\"./out\"\n",
    "\n",
    "# monkey patch MambaLMHeadModel.forward \n",
    "def forward_with_loss(self, input_ids, position_ids=None, inference_params=None, num_last_tokens=0, labels = None):\n",
    "    \"\"\"\n",
    "    \"position_ids\" is just to be compatible with Transformer generation. We don't use it.\n",
    "    num_last_tokens: if > 0, only return the logits for the last n tokens\n",
    "    \"\"\"\n",
    "    hidden_states = self.backbone(input_ids, inference_params=inference_params)\n",
    "    if num_last_tokens > 0:\n",
    "        hidden_states = hidden_states[:, -num_last_tokens:]\n",
    "    lm_logits = self.lm_head(hidden_states)\n",
    "    \n",
    "    # Source: https://github.com/huggingface/transformers/blob/80377eb018c077dba434bc8e7912bcaed3a64d09/src/transformers/models/llama/modeling_llama.py#L1196\n",
    "    from torch.nn import CrossEntropyLoss\n",
    "    if labels is not None:\n",
    "        logits = lm_logits\n",
    "        # Shift so that tokens < n predict n\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "        # Flatten the tokens\n",
    "        loss_fct = CrossEntropyLoss()\n",
    "        # shift_logits = shift_logits.view(-1, self.config.vocab_size)\n",
    "        shift_logits = shift_logits.view(-1, self.backbone.embedding.weight.size()[0])\n",
    "        shift_labels = shift_labels.view(-1)\n",
    "        # Enable model parallelism\n",
    "        shift_labels = shift_labels.to(shift_logits.device)\n",
    "        loss = loss_fct(shift_logits, shift_labels)\n",
    "        return (loss,)   \n",
    "    else:\n",
    "        CausalLMOutput = namedtuple(\"CausalLMOutput\", [\"logits\"])\n",
    "        return CausalLMOutput(logits=lm_logits)\n",
    "MambaLMHeadModel.forward=forward_with_loss\n",
    "\n",
    "# Load model\n",
    "model = MambaLMHeadModel.from_pretrained(\n",
    "    modelpath,    \n",
    "    dtype=torch.bfloat16,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "# Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\") \n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Add ChatML tokens to tokenizer and model\n",
    "def resize_token_embeddings(model, new_num_tokens):\n",
    "    import torch.nn as nn\n",
    "\n",
    "    old_embeddings = model.backbone.embedding\n",
    "    old_num_tokens, old_embedding_dim = old_embeddings.weight.size()\n",
    "    new_embeddings = nn.Embedding(\n",
    "        new_num_tokens,\n",
    "        old_embedding_dim,\n",
    "        device=old_embeddings.weight.device,\n",
    "        dtype=old_embeddings.weight.dtype,\n",
    "    )\n",
    "    nn.init.normal_(new_embeddings.weight, std=0.02)\n",
    "    n = min(old_num_tokens, new_num_tokens)\n",
    "    new_embeddings.weight.data[:n, :] = old_embeddings.weight.data[:n, :]\n",
    "    model.backbone.embedding = new_embeddings\n",
    "\n",
    "    model.tie_weights()\n",
    "\n",
    "tokenizer.add_tokens([\"<PAD>\"])\n",
    "tokenizer.add_tokens([\"<|im_start|>\"])\n",
    "tokenizer.add_special_tokens(dict(eos_token=\"<|im_end|>\"))\n",
    "tokenizer.pad_token = \"<PAD>\"\n",
    "tokenizer.eos_token=\"<|im_end|>\"\n",
    "resize_token_embeddings(model, len(tokenizer))\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer.save_pretrained(f\"{output_dir}/tokenizer/\")\n",
    "\n",
    "# Load dataset\n",
    "dataset_name=\"OpenAssistant/oasst_top1_2023-08-25\"\n",
    "dataset=load_dataset(dataset_name)\n",
    "\n",
    "# Tokenize dataset\n",
    "def tokenize(element):\n",
    "    return tokenizer(\n",
    "        element[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "\n",
    "dataset_tokenized = dataset.map(\n",
    "    tokenize, \n",
    "    batched=True, \n",
    "    num_proc=os.cpu_count(),    # multithreaded\n",
    "    remove_columns=[\"text\"]     # don't need this anymore, we have tokens from here on\n",
    ")\n",
    "\n",
    "# collate function - to transform list of dictionaries [ {input_ids: [123, ..]}, {.. ] to single batch dictionary { input_ids: [..], labels: [..], attention_mask: [..] }\n",
    "def collate(elements):\n",
    "    tokenlist=[e[\"input_ids\"] for e in elements]\n",
    "    tokens_maxlen=max([len(t) for t in tokenlist])\n",
    "\n",
    "    input_ids,labels = [],[]\n",
    "    for tokens in tokenlist:\n",
    "        pad_len=tokens_maxlen-len(tokens)\n",
    "\n",
    "        # pad input_ids with pad_token, labels with ignore_index (-100) \n",
    "        input_ids.append( tokens + [tokenizer.pad_token_id]*pad_len )   \n",
    "        labels.append( tokens + [-100]*pad_len )    \n",
    "    batch={\n",
    "        \"input_ids\": torch.tensor(input_ids),\n",
    "        \"labels\": torch.tensor(labels),\n",
    "    }\n",
    "    return batch\n",
    "\n",
    "run_name=\"{model}_{ds}_BS-{bs}_LR-{lr}\".format(\n",
    "    model=modelpath.split(\"/\")[1],\n",
    "    ds=dataset_name.split(\"/\")[1],\n",
    "    bs=bs,\n",
    "    lr=lr,\n",
    "    )\n",
    "run_name+=\"-ChatML\"\n",
    "\n",
    "steps_per_epoch=len(dataset_tokenized[\"train\"])//(accelerator.state.num_processes*bs*ga_steps)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=bs,\n",
    "    per_device_eval_batch_size=bs,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_steps=1,\n",
    "    eval_steps=steps_per_epoch,\n",
    "    save_steps=steps_per_epoch,\n",
    "    gradient_accumulation_steps=ga_steps,\n",
    "    num_train_epochs=epochs,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    learning_rate=lr,\n",
    "    group_by_length=True,\n",
    "    bf16=True,\n",
    "    ddp_find_unused_parameters=False,\n",
    "    save_safetensors=False,\n",
    "    run_name=run_name\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=collate,\n",
    "    train_dataset=dataset_tokenized[\"train\"],\n",
    "    eval_dataset=dataset_tokenized[\"test\"],\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

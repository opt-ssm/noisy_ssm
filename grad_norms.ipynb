{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import sys\n",
    "from contextlib import contextmanager\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @contextmanager\n",
    "# def suppress_output():\n",
    "#     with open(os.devnull, 'w') as fnull:\n",
    "#         old_stdout = sys.stdout\n",
    "#         old_stderr = sys.stderr\n",
    "#         sys.stdout = fnull\n",
    "#         sys.stderr = fnull\n",
    "#         try:\n",
    "#             yield\n",
    "#         finally:\n",
    "#             sys.stdout = old_stdout\n",
    "#             sys.stderr = old_stderr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"training.log\"),  \n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sapr7y/noisy_ssm/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-09-02 21:17:45.630384: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-02 21:17:45.650534: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-02 21:17:45.656915: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-02 21:17:45.673492: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-02 21:17:46.810081: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoTokenizer, TrainingArguments, AutoModelForCausalLM\n",
    "from mamba_trainer.data import DataModule\n",
    "from mamba_trainer.trainer import MambaTrainer, GradientCallback\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"state-spaces/mamba-130m-hf\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-130m-hf\")\n",
    "tokenizer.eos_token = \"<|endoftext|>\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    target_modules=[\"x_proj\", \"embeddings\", \"in_proj\", \"out_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): MambaForCausalLM(\n",
       "      (backbone): MambaModel(\n",
       "        (embeddings): lora.Embedding(\n",
       "          (base_layer): Embedding(50280, 768)\n",
       "          (lora_dropout): ModuleDict(\n",
       "            (default): Identity()\n",
       "          )\n",
       "          (lora_A): ModuleDict()\n",
       "          (lora_B): ModuleDict()\n",
       "          (lora_embedding_A): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 16x50280 (cuda:0)])\n",
       "          (lora_embedding_B): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 768x16 (cuda:0)])\n",
       "          (lora_magnitude_vector): ModuleDict()\n",
       "        )\n",
       "        (layers): ModuleList(\n",
       "          (0-23): 24 x MambaBlock(\n",
       "            (norm): MambaRMSNorm(768, eps=1e-05)\n",
       "            (mixer): MambaMixer(\n",
       "              (conv1d): Conv1d(1536, 1536, kernel_size=(4,), stride=(1,), padding=(3,), groups=1536)\n",
       "              (act): SiLU()\n",
       "              (in_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (x_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=80, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=80, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (dt_proj): Linear(in_features=48, out_features=1536, bias=True)\n",
       "              (out_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=768, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (norm_f): MambaRMSNorm(768, eps=1e-05)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=768, out_features=50280, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,796,608 || all params: 132,931,968 || trainable%: 2.86\n"
     ]
    }
   ],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params:,} || all params: {all_param:,} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
    "    )\n",
    "\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sapr7y/noisy_ssm/.venv/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    output_dir=\"model\",\n",
    "    logging_dir=\"logs\",\n",
    "    evaluation_strategy=\"no\", \n",
    "    logging_steps=1,\n",
    "    save_steps=1,\n",
    "    report_to=\"none\",\n",
    "    disable_tqdm=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/basic_50-150/train.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1725301074.160989 3167098 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1725301074.165349 3167098 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1725301074.167017 3167098 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1725301074.205443 3167098 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1725301074.210911 3167098 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1725301074.213349 3167098 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1725301074.215564 3167098 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1725301074.218181 3167098 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1725301074.219801 3167098 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-02 21:17:54.228470: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 78634 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:05:00.0, compute capability: 8.0\n",
      "2024-09-02 21:18:02.549102: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/basic_50-150/val.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-02 21:18:04.077800: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "train_data_module = DataModule(data_path=\"./data/basic_50-150/train.tsv\", tokenizer=tokenizer)\n",
    "train_dataset = train_data_module.dataset\n",
    "val_data_module = DataModule(data_path=\"./data/basic_50-150/val.tsv\", tokenizer=tokenizer)\n",
    "val_dataset = val_data_module.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(trainer, eval_dataset, step):\n",
    "    metrics = trainer.evaluate(eval_dataset)\n",
    "    loss = metrics.get('eval_loss', None)\n",
    "    logger.info(f\"Validation Loss at step {step}: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1000 [00:01<30:44,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1.235, 'train_samples_per_second': 3.239, 'train_steps_per_second': 0.81, 'train_loss': 2.2836382389068604, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1000 [00:02<41:05,  2.47s/it]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 226.00 MiB. GPU 0 has a total capacty of 79.26 GiB of which 104.75 MiB is free. Including non-PyTorch memory, this process has 79.14 GiB memory in use. Of the allocated memory 1.72 GiB is allocated by PyTorch, and 132.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 19\u001b[0m\n\u001b[1;32m      7\u001b[0m subset \u001b[38;5;241m=\u001b[39m Subset(train_dataset, ids\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[1;32m      9\u001b[0m trainer \u001b[38;5;241m=\u001b[39m MambaTrainer(\n\u001b[1;32m     10\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     11\u001b[0m         args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39m[grad_callback]\n\u001b[1;32m     17\u001b[0m     )\n\u001b[0;32m---> 19\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m norms\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(grad_callback\u001b[38;5;241m.\u001b[39mgradients))\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grad_callback\u001b[38;5;241m.\u001b[39mstep \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m20\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/noisy_ssm/.venv/lib/python3.11/site-packages/transformers/trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1936\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1939\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/noisy_ssm/.venv/lib/python3.11/site-packages/transformers/trainer.py:2279\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2278\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2279\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2282\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2283\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2284\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2285\u001b[0m ):\n\u001b[1;32m   2286\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2287\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/noisy_ssm/.venv/lib/python3.11/site-packages/transformers/trainer.py:3318\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3317\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3318\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3320\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3323\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3324\u001b[0m ):\n",
      "File \u001b[0;32m~/noisy_ssm/mamba_trainer/trainer.py:21\u001b[0m, in \u001b[0;36mMambaTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m     18\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     23\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n",
      "File \u001b[0;32m~/noisy_ssm/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/noisy_ssm/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/noisy_ssm/.venv/lib/python3.11/site-packages/peft/peft_model.py:1577\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1575\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1576\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1577\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1578\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1579\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1580\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1581\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1582\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1584\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1585\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1586\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1588\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1589\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1590\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m~/noisy_ssm/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/noisy_ssm/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/noisy_ssm/.venv/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:188\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/noisy_ssm/.venv/lib/python3.11/site-packages/transformers/models/mamba/modeling_mamba.py:760\u001b[0m, in \u001b[0;36mMambaForCausalLM.forward\u001b[0;34m(self, input_ids, inputs_embeds, cache_params, labels, output_hidden_states, return_dict, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;66;03m# Flatten the tokens\u001b[39;00m\n\u001b[1;32m    759\u001b[0m     loss_fct \u001b[38;5;241m=\u001b[39m CrossEntropyLoss()\n\u001b[0;32m--> 760\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshift_logits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshift_logits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshift_labels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m    763\u001b[0m     output \u001b[38;5;241m=\u001b[39m (logits,) \u001b[38;5;241m+\u001b[39m mamba_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m~/noisy_ssm/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/noisy_ssm/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/noisy_ssm/.venv/lib/python3.11/site-packages/torch/nn/modules/loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/noisy_ssm/.venv/lib/python3.11/site-packages/torch/nn/functional.py:3053\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3052\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 226.00 MiB. GPU 0 has a total capacty of 79.26 GiB of which 104.75 MiB is free. Including non-PyTorch memory, this process has 79.14 GiB memory in use. Of the allocated memory 1.72 GiB is allocated by PyTorch, and 132.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "grad_callback = GradientCallback()\n",
    "norms = []\n",
    "\n",
    "for i in tqdm(range(1000), disable=False):\n",
    "    np.random.seed(None)\n",
    "    ids = np.random.choice(len(train_dataset), size=4, replace=False)\n",
    "    subset = Subset(train_dataset, ids.tolist())\n",
    "    \n",
    "    trainer = MambaTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=subset,\n",
    "            tokenizer=tokenizer,\n",
    "            optimizers=(optimizer, None),\n",
    "            data_collator=train_data_module.data_collator,\n",
    "            callbacks=[grad_callback]\n",
    "        )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "\n",
    "    norms.append(torch.linalg.norm(grad_callback.gradients))\n",
    "\n",
    "    if grad_callback.step % 20 == 0:\n",
    "        evaluate(trainer=trainer,\n",
    "                     eval_dataset=val_dataset,\n",
    "                     step=grad_callback.step\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
